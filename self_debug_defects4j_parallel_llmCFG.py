#!/usr/bin/env python3
"""
ä½¿ç”¨LLMç›´æ¥ç”ŸæˆJavaæ§åˆ¶æµå›¾çš„è‡ªè°ƒè¯•æ¶æ„
åŸºäºself_debug_defects4j_parallel.pyä½†ä½¿ç”¨LLMä»£æ›¿JavaCFGæ„å»ºæ§åˆ¶æµå›¾
"""

import json
import os
import time
import random
import argparse
import re
from typing import Dict, List, Optional, Tuple
from loguru import logger
import subprocess
import sys
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
from utils import write_str_to_file
from chat import chat_java_fragment_debug, generate_java_cfg_with_llm

def slim_error_message(err_msg: str, token_limit: int = 200) -> str:
    """
    ç®€åŒ–error messageï¼Œç±»ä¼¼gen_solution_prompt.pyä¸­çš„slim_content_token
    Args:
        err_msg: åŸå§‹é”™è¯¯ä¿¡æ¯
        token_limit: tokené™åˆ¶
    Returns:
        ç®€åŒ–åçš„é”™è¯¯ä¿¡æ¯
    """
    err_msg_lines = err_msg.split('\n')
    slim_err_msg_lines = []
    current_tokens = 0
    
    for line in err_msg_lines:
        # ç®€å•ä¼°ç®—ï¼šä¸€ä¸ªå•è¯çº¦ç­‰äº1ä¸ªtoken
        line_tokens = len(line.split())
        if current_tokens + line_tokens > token_limit:
            break
        slim_err_msg_lines.append(line)
        current_tokens += line_tokens
    
    return '\n'.join(slim_err_msg_lines)

def extract_java_buggy_code(bug_data: Dict) -> str:
    """
    æå–Javaçš„buggyä»£ç 
    Args:
        bug_data: å•ä¸ªbugçš„æ•°æ®
    Returns:
        å®Œæ•´çš„buggyä»£ç 
    """
    buggy_code = bug_data['buggy']
    buggy_code_comment = bug_data.get('buggy_code_comment', '')
    
    # ç»„åˆæ³¨é‡Šå’Œä»£ç 
    if buggy_code_comment:
        full_code = buggy_code_comment + '\n' + buggy_code
    else:
        full_code = buggy_code
    
    return full_code

def extract_java_test_info(bug_data: Dict) -> Tuple[str, str]:
    """
    ä»trigger_testä¸­éšæœºé€‰æ‹©ä¸€ä¸ªæµ‹è¯•ç”¨ä¾‹å’Œé”™è¯¯ä¿¡æ¯
    Args:
        bug_data: å•ä¸ªbugçš„æ•°æ®
    Returns:
        (test_case, error_message) å…ƒç»„
    """
    trigger_tests = bug_data.get('trigger_test', {})
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªtrigger test
    if trigger_tests:
        random_trigger_test = random.choice(list(trigger_tests.keys()))
        selected_test = trigger_tests[random_trigger_test]
        test_case = selected_test.get('src', '')
        error_message = selected_test.get('clean_error_msg', '')
        
        if error_message:
            error_message = slim_error_message(error_message)
        
        return test_case, error_message
    
    return "", ""

def llm_java_debug(bug_name: str, bug_data: Dict) -> Optional[str]:
    """
    ä½¿ç”¨LLMç”Ÿæˆæ§åˆ¶æµå›¾å¹¶è¿›è¡ŒJavaä»£ç è°ƒè¯•
    Args:
        bug_name: bugåç§°
        bug_data: bugæ•°æ®
    Returns:
        ä¿®å¤åçš„ä»£ç ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    logger.info(f"Processing bug: {bug_name}")
    
    # æå–åŸºæœ¬ä¿¡æ¯
    buggy_code = extract_java_buggy_code(bug_data)
    test_case, error_message = extract_java_test_info(bug_data)
    
    logger.info(f"Buggy code length: {len(buggy_code)}")
    logger.info(f"Test case length: {len(test_case)}")
    logger.info(f"Error message length: {len(error_message)}")
    
    # æ„å»ºCFG - ä½¿ç”¨LLMç”Ÿæˆ
    cfg_text = ""
    try:
        # æ£€æŸ¥ä»£ç æ˜¯å¦åŒ…å«ç±»å®šä¹‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™åŒ…è£…åœ¨ä¸´æ—¶ç±»ä¸­
        java_code_to_analyze = buggy_code
        class_name = None
        
        if not re.search(r'\bclass\s+\w+', buggy_code):
            # æ²¡æœ‰ç±»å®šä¹‰ï¼ŒåŒ…è£…åœ¨ä¸´æ—¶ç±»ä¸­
            class_name = "TempClass"
            java_code_to_analyze = f"""
public class TempClass {{
{buggy_code}
}}
"""
            logger.info(f"Wrapped method in temporary class for {bug_name}")
        
        # ä½¿ç”¨LLMç”Ÿæˆæ§åˆ¶æµå›¾
        logger.info(f"Generating CFG with LLM for {bug_name}...")
        cfg_text = generate_java_cfg_with_llm(
            java_code=java_code_to_analyze, 
            method_name=None,  # ä¸é™åˆ¶ç‰¹å®šæ–¹æ³•ï¼Œè®©LLMåˆ†ææ•´ä¸ªä»£ç 
            class_name=class_name
        )
        logger.info(f"CFG generated successfully for {bug_name}")
        
    except Exception as e:
        logger.warning(f"CFG generation failed for {bug_name}: {e}")
        cfg_text = ""
    
    # ä½¿ç”¨é™æ€åˆ†ææ–¹æ³•è¿›è¡Œè°ƒè¯•
    try:
        logger.info(f"Starting static analysis debug for {bug_name}")
        
        # å¦‚æœæ²¡æœ‰æµ‹è¯•ç”¨ä¾‹æˆ–é”™è¯¯ä¿¡æ¯ï¼Œä½¿ç”¨å ä½ç¬¦
        if not test_case:
            test_case = "No specific test case available"
        if not error_message:
            error_message = "No specific error message available"
            
        debug_result = chat_java_fragment_debug(
            buggy_code=buggy_code,
            error_message=error_message,
            test_case=test_case,
            cfg_text=cfg_text
        )
        
        # æ‰“å°åŸå§‹å“åº”ç”¨äºè°ƒè¯•
        logger.info(f"Raw LLM response for {bug_name}:")
        logger.info(f"Response length: {len(debug_result)}")
        logger.info(f"First 500 chars: {debug_result[:500]}")
        
        # é¢„å¤„ç†å“åº”ï¼Œå»æ‰markdownä»£ç å—æ ‡è®°
        processed_result = debug_result.strip()
        if processed_result.startswith("```json"):
            processed_result = processed_result[7:]  # å»æ‰```json
        if processed_result.endswith("```"):
            processed_result = processed_result[:-3]  # å»æ‰```
        processed_result = processed_result.strip()
        
        # è§£æç»“æœ
        try:
            debug_json = json.loads(processed_result)
            corrected_code = debug_json.get("corrected_code", buggy_code)
            explanation = debug_json.get("explanation", "No explanation provided")
            
            logger.info(f"Debug completed for {bug_name}")
            logger.info(f"Explanation: {explanation[:200]}...")
            
            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ä¿®å¤ä»£ç ï¼ˆä¸ç®¡æ˜¯å¦æ­£ç¡®ï¼Œéƒ½éœ€è¦éªŒè¯ï¼‰
            if corrected_code and corrected_code.strip() != buggy_code.strip():
                logger.info(f"ğŸ“ Generated patch for {bug_name} (needs validation)")
                return corrected_code
            else:
                logger.warning(f"âŒ No patch generated for {bug_name}")
                return None
                
        except json.JSONDecodeError as e:
            logger.warning(f"JSON parse error for {bug_name}: {e}")
            logger.warning(f"Trying to extract code from non-JSON response...")
            
            # å°è¯•ä»åŸå§‹å“åº”ä¸­æå–ä»£ç 
            if "```java" in debug_result:
                start = debug_result.find("```java") + 7
                end = debug_result.find("```", start)
                if end > start:
                    extracted_code = debug_result[start:end].strip()
                    if extracted_code and extracted_code != buggy_code.strip():
                        logger.info(f"ğŸ“ Extracted patch from non-JSON response for {bug_name} (needs validation)")
                        return extracted_code
            
            logger.warning(f"âŒ Could not extract any meaningful fix for {bug_name}")
            return None
            
    except Exception as e:
        logger.error(f"Static analysis debug failed for {bug_name}: {e}")
        return None

def process_single_bug_task(task_data: Tuple[str, Dict]) -> Tuple[str, Optional[str], bool]:
    """
    å¹¶è¡Œå¤„ç†å•ä¸ªbugä»»åŠ¡
    Args:
        task_data: (bug_name, bug_data) å…ƒç»„
    Returns:
        (bug_name, corrected_code, success) å…ƒç»„
    """
    bug_name, bug_data = task_data
    
    try:
        corrected_code = llm_java_debug(bug_name, bug_data)
        success = corrected_code is not None and corrected_code.strip() != bug_data['buggy'].strip()
        return bug_name, corrected_code, success
    except Exception as e:
        logger.error(f"Error processing {bug_name}: {e}")
        return bug_name, None, False

def process_defects4j_dataset_parallel(dataset_path: str, output_path: str, limit: int = None, max_workers: int = None) -> Dict:
    """
    å¹¶è¡Œå¤„ç†æ•´ä¸ªdefects4jæ•°æ®é›†
    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„
        limit: é™åˆ¶å¤„ç†çš„bugæ•°é‡
        max_workers: æœ€å¤§å¹¶è¡Œworkeræ•°é‡
    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    logger.info(f"Loading dataset from {dataset_path}")
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    total_bugs = len(dataset)
    logger.info(f"Total bugs in dataset: {total_bugs}")
    
    bug_names = list(dataset.keys())
    
    # å¦‚æœè®¾ç½®äº†é™åˆ¶ï¼Œåªå¤„ç†æŒ‡å®šæ•°é‡çš„bugs
    if limit is not None and limit > 0:
        bug_names = bug_names[:limit]
        logger.info(f"Limited processing to first {limit} bugs")
    
    # è®¾ç½®å¹¶è¡Œworkeræ•°é‡
    if max_workers is None:
        max_workers = min(multiprocessing.cpu_count(), len(bug_names))
    
    logger.info(f"Using {max_workers} parallel workers")
    
    # å‡†å¤‡ä»»åŠ¡æ•°æ®
    tasks = [(bug_name, dataset[bug_name]) for bug_name in bug_names]
    
    results = {}
    patches_generated = 0
    
    # å¹¶è¡Œå¤„ç†
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_bug = {executor.submit(process_single_bug_task, task): task[0] for task in tasks}
        
        # æ”¶é›†ç»“æœ
        for i, future in enumerate(as_completed(future_to_bug), 1):
            bug_name = future_to_bug[future]
            
            try:
                bug_name_result, corrected_code, success = future.result()
                
                logger.info(f"=== Completed bug {i}/{len(bug_names)}: {bug_name_result} ===")
                
                if corrected_code and corrected_code != dataset[bug_name_result]['buggy']:
                    results[bug_name_result] = {
                        'patches': [corrected_code],
                        'original_buggy': dataset[bug_name_result]['buggy'],
                        'bug_info': {
                            'loc': dataset[bug_name_result]['loc'],
                            'start': dataset[bug_name_result]['start'],
                            'end': dataset[bug_name_result]['end']
                        },
                        'patch_generated': True
                    }
                    patches_generated += 1
                    logger.info(f"ğŸ“ Generated patch for {bug_name_result} (validation required)")
                else:
                    logger.warning(f"âŒ No patch generated for {bug_name_result}")
                    # ä¸ºäº†èƒ½å¤Ÿè¿›è¡ŒéªŒè¯ï¼Œå³ä½¿å¤±è´¥ä¹Ÿè¦è®°å½•åŸå§‹ä»£ç 
                    results[bug_name_result] = {
                        'patches': [dataset[bug_name_result]['buggy']],  # ä½¿ç”¨åŸå§‹ä»£ç 
                        'original_buggy': dataset[bug_name_result]['buggy'],
                        'bug_info': {
                            'loc': dataset[bug_name_result]['loc'],
                            'start': dataset[bug_name_result]['start'],
                            'end': dataset[bug_name_result]['end']
                        },
                        'patch_generated': False
                    }
                
                # å®šæœŸä¿å­˜ä¸­é—´ç»“æœ
                if i % 10 == 0:
                    logger.info(f"Progress: {i}/{len(bug_names)} completed, saving intermediate results...")
                    with open(output_path, 'w', encoding='utf-8') as f:
                        json.dump(results, f, indent=2, ensure_ascii=False)
                        
            except Exception as e:
                logger.error(f"Error processing result for {bug_name}: {e}")
                # è®°å½•å¤±è´¥çš„æƒ…å†µ
                results[bug_name] = {
                    'patches': [dataset[bug_name]['buggy']],  # ä½¿ç”¨åŸå§‹ä»£ç 
                    'original_buggy': dataset[bug_name]['buggy'],
                    'bug_info': {
                        'loc': dataset[bug_name]['loc'],
                        'start': dataset[bug_name]['start'],
                        'end': dataset[bug_name]['end']
                    },
                    'patch_generated': False
                }
    
    logger.info(f"=== Parallel processing completed ===")
    logger.info(f"Total processed: {len(bug_names)}")
    logger.info(f"Patches generated: {patches_generated}")
    logger.info(f"Patch generation rate: {patches_generated/len(bug_names)*100:.2f}%")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Results saved to {output_path}")
    return results

def run_validation(patch_file: str, dataset_path: str, output_dir: str):
    """
    è¿è¡Œsf_val_d4jéªŒè¯ï¼Œæ˜¾ç¤ºå®æ—¶è¿›åº¦
    Args:
        patch_file: è¡¥ä¸æ–‡ä»¶è·¯å¾„
        dataset_path: æ•°æ®é›†è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
    """
    logger.info("Starting validation with sf_val_d4j...")
    
    # æ„å»ºéªŒè¯å‘½ä»¤
    val_script = "dataset_test/SRepair/SRepair/src/sf_val_d4j.py"
    
    if not os.path.exists(val_script):
        logger.error(f"Validation script not found: {val_script}")
        return
    
    # è¯»å–è¡¥ä¸æ–‡ä»¶è·å–æ€»æ•°é‡ä»¥æ˜¾ç¤ºè¿›åº¦
    try:
        with open(patch_file, 'r', encoding='utf-8') as f:
            patches_data = json.load(f)
        total_bugs = len(patches_data)
        logger.info(f"ğŸ“Š Total bugs to validate: {total_bugs}")
    except Exception as e:
        logger.error(f"Error reading patch file: {e}")
        total_bugs = 0
    
    cmd = [
        sys.executable, val_script,
        '-i', patch_file,
        '-d', dataset_path,
        '-o', output_dir
    ]
    
    logger.info(f"Running validation command: {' '.join(cmd)}")
    
    try:
        # å¯åŠ¨éªŒè¯è¿›ç¨‹å¹¶å®æ—¶æ˜¾ç¤ºè¾“å‡º
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, 
                                 text=True, bufsize=1, universal_newlines=True)
        
        # å¯åŠ¨è¿›åº¦ç›‘æ§çº¿ç¨‹
        import threading
        stop_monitoring = threading.Event()
        monitor_thread = threading.Thread(target=monitor_validation_progress, 
                                        args=(output_dir, total_bugs, stop_monitoring))
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # å®æ—¶è¾“å‡ºéªŒè¯æ—¥å¿—
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                # è¿‡æ»¤å’Œæ ¼å¼åŒ–è¾“å‡º
                line = output.strip()
                if line:
                    if '[PATCH STATUS]' in line:
                        logger.info(f"ğŸ” {line}")
                    elif '[TIME INFO]' in line:
                        logger.info(f"â±ï¸ {line}")
                    elif '[CHECKOUT]' in line:
                        logger.info(f"ğŸ“¦ {line}")
                    elif 'END VALIDATION' in line:
                        logger.info(f"âœ… {line}")
                    else:
                        logger.debug(f"[VAL] {line}")
        
        # åœæ­¢ç›‘æ§å¹¶ç­‰å¾…è¿›ç¨‹å®Œæˆ
        stop_monitoring.set()
        return_code = process.wait()
        
        if return_code == 0:
            logger.info("âœ… Validation completed successfully!")
            logger.info(f"ğŸ“ Results saved to: {output_dir}")
        else:
            logger.error(f"âŒ Validation failed with return code {return_code}")
            
    except subprocess.TimeoutExpired:
        logger.error("âŒ Validation timed out after 1 hour")
    except Exception as e:
        logger.error(f"âŒ Error running validation: {e}")

def monitor_validation_progress(output_dir: str, total_bugs: int, stop_event):
    """
    ç›‘æ§éªŒè¯è¿›åº¦
    """
    import time
    
    if total_bugs == 0:
        return
    
    start_time = time.time()
    last_count = 0
    
    while not stop_event.is_set():
        try:
            if not os.path.exists(output_dir):
                time.sleep(5)
                continue
            
            # ç»Ÿè®¡å·²å®Œæˆçš„éªŒè¯æ–‡ä»¶
            import glob
            completed_files = glob.glob(os.path.join(output_dir, '*-validated.jsonl'))
            completed_count = len(completed_files)
            
            if completed_count > last_count:
                elapsed_time = time.time() - start_time
                progress_percent = (completed_count / total_bugs) * 100
                
                if completed_count > 0:
                    avg_time_per_bug = elapsed_time / completed_count
                    remaining_bugs = total_bugs - completed_count
                    eta_seconds = avg_time_per_bug * remaining_bugs
                    eta_minutes = eta_seconds / 60
                    
                    logger.info(f"ğŸ“ˆ Progress: {completed_count}/{total_bugs} ({progress_percent:.1f}%) "
                              f"| Elapsed: {elapsed_time/60:.1f}m | ETA: {eta_minutes:.1f}m")
                
                last_count = completed_count
            
            if completed_count >= total_bugs:
                break
                
            time.sleep(15)  # æ¯15ç§’æ£€æŸ¥ä¸€æ¬¡
            
        except Exception as e:
            logger.debug(f"Progress monitoring error: {e}")
            time.sleep(15)

def parse_validation_results(validation_output_dir: str) -> Dict:
    """
    è§£æéªŒè¯ç»“æœå¹¶ç»Ÿè®¡ä¿®å¤æ­£ç¡®ç‡ï¼Œå¹¶æŒ‰é¡¹ç›®ç±»å‹åˆ†ç±»ç»Ÿè®¡
    Args:
        validation_output_dir: éªŒè¯ç»“æœè¾“å‡ºç›®å½•
    Returns:
        ç»Ÿè®¡ç»“æœå­—å…¸
    """
    logger.info(f"Parsing validation results from {validation_output_dir}")
    
    if not os.path.exists(validation_output_dir):
        logger.error(f"Validation output directory not found: {validation_output_dir}")
        return {}
    
    validation_files = [f for f in os.listdir(validation_output_dir) if f.endswith('-validated.jsonl')]
    
    total_bugs = 0
    plausible_fixes = 0
    correct_fixes = 0
    uncompilable_fixes = 0
    timeout_fixes = 0
    
    # æŒ‰é¡¹ç›®ç±»å‹åˆ†ç±»çš„ç»“æœ
    project_stats = {}
    
    detailed_results = {}
    
    for val_file in validation_files:
        val_file_path = os.path.join(validation_output_dir, val_file)
        bug_name = val_file.replace('-validated.jsonl', '')
        
        # æå–é¡¹ç›®ç±»å‹ï¼ˆå¦‚Chartã€Cliã€Closureç­‰ï¼‰
        project_type = None
        if "-" in bug_name:
            project_type = bug_name.split("-")[0]
        
        # åˆå§‹åŒ–é¡¹ç›®ç±»å‹çš„ç»Ÿè®¡æ•°æ®
        if project_type and project_type not in project_stats:
            project_stats[project_type] = {
                'total': 0,
                'plausible': 0,
                'correct': 0,
                'uncompilable': 0,
                'timeout': 0,
                'other': 0,
                'bugs': []
            }
        
        try:
            with open(val_file_path, 'r', encoding='utf-8') as f:
                bug_results = json.load(f)
            
            for patch_result in bug_results:
                total_bugs += 1
                status = patch_result.get('patch_status', 'UNKNOWN')
                
                # æ›´æ–°é¡¹ç›®ç±»å‹ç»Ÿè®¡
                if project_type:
                    project_stats[project_type]['total'] += 1
                    project_stats[project_type]['bugs'].append(bug_name)
                
                detailed_results[f"{bug_name}_patch_{patch_result.get('val_cnt', 1)}"] = {
                    'bug_name': bug_name,
                    'project_type': project_type,
                    'status': status,
                    'failing_tests': patch_result.get('failing_tests', {}),
                    'patch_code': patch_result.get('patch_code', '')[:100] + '...'  # åªä¿ç•™å‰100å­—ç¬¦
                }
                
                if status == 'PLAUSIBLE':
                    plausible_fixes += 1
                    correct_fixes += 1  # PLAUSIBLE è¡¨ç¤ºé€šè¿‡äº†æ‰€æœ‰æµ‹è¯•
                    if project_type:
                        project_stats[project_type]['plausible'] += 1
                        project_stats[project_type]['correct'] += 1
                elif status == 'UNCOMPILABLE':
                    uncompilable_fixes += 1
                    if project_type:
                        project_stats[project_type]['uncompilable'] += 1
                elif 'TIMEOUT' in status:
                    timeout_fixes += 1
                    if project_type:
                        project_stats[project_type]['timeout'] += 1
                else:
                    if project_type:
                        project_stats[project_type]['other'] += 1
        
        except Exception as e:
            logger.error(f"Error parsing validation file {val_file}: {e}")
            continue
    
    # è®¡ç®—ç»Ÿè®¡ç»“æœ
    plausible_rate = 0
    correct_rate = 0
    
    if total_bugs > 0:
        plausible_rate = (plausible_fixes / total_bugs) * 100
        correct_rate = (correct_fixes / total_bugs) * 100
    
    # è®¡ç®—æ¯ä¸ªé¡¹ç›®ç±»å‹çš„ä¿®å¤ç‡
    for project, stats in project_stats.items():
        if stats['total'] > 0:
            stats['plausible_rate'] = round((stats['plausible'] / stats['total']) * 100, 2)
            stats['correct_rate'] = round((stats['correct'] / stats['total']) * 100, 2)
            # ç§»é™¤é‡å¤çš„bugåˆ—è¡¨ï¼Œåªä¿ç•™å”¯ä¸€å€¼
            stats['bugs'] = list(set(stats['bugs']))
            stats['bug_count'] = len(stats['bugs'])
    
    # æŒ‰ç…§é¡¹ç›®ä¿®å¤ç‡æ’åº
    sorted_projects = sorted(project_stats.items(), 
                            key=lambda x: x[1]['correct_rate'] if x[1]['total'] > 0 else 0, 
                            reverse=True)
    
    # æ„å»ºæ’åºåçš„é¡¹ç›®ç»Ÿè®¡
    sorted_project_stats = {k: v for k, v in sorted_projects}
    
    statistics = {
        'total_bugs_validated': total_bugs,
        'plausible_fixes': plausible_fixes,
        'correct_fixes': correct_fixes,
        'uncompilable_fixes': uncompilable_fixes,
        'timeout_fixes': timeout_fixes,
        'other_fixes': total_bugs - plausible_fixes - uncompilable_fixes - timeout_fixes,
        'plausible_rate': round(plausible_rate, 2),
        'correct_rate': round(correct_rate, 2),
        'project_statistics': sorted_project_stats,
        'detailed_results': detailed_results
    }
    
    # æ‰“å°æ€»ä½“ç»Ÿè®¡ç»“æœ
    logger.info("=== DEFECTS4J REPAIR STATISTICS ===")
    logger.info(f"Total bugs validated: {total_bugs}")
    logger.info(f"Plausible fixes: {plausible_fixes}")
    logger.info(f"Correct fixes: {correct_fixes}")
    logger.info(f"Uncompilable fixes: {uncompilable_fixes}")
    logger.info(f"Timeout fixes: {timeout_fixes}")
    logger.info(f"Other status fixes: {statistics['other_fixes']}")
    logger.info(f"Plausible rate: {plausible_rate:.2f}%")
    logger.info(f"Correct rate: {correct_rate:.2f}%")
    
    # æ‰“å°æ¯ä¸ªé¡¹ç›®ç±»å‹çš„ç»Ÿè®¡ç»“æœ
    logger.info("\n=== PROJECT TYPE STATISTICS ===")
    for project, stats in sorted_projects:
        if stats['total'] > 0:
            logger.info(f"Project {project}:")
            logger.info(f"  Total bugs: {stats['bug_count']} (with {stats['total']} validation attempts)")
            logger.info(f"  Plausible fixes: {stats['plausible']} ({stats['plausible_rate']}%)")
            logger.info(f"  Correct fixes: {stats['correct']} ({stats['correct_rate']}%)")
            logger.info(f"  Uncompilable: {stats['uncompilable']}, Timeout: {stats['timeout']}, Other: {stats['other']}")
    
    logger.info("=" * 40)
    
    return statistics

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Use LLM-based CFG generation for Java code debugging")
    parser.add_argument('--dataset', '-d', type=str, 
                       default='dataset_test/SRepair/SRepair/dataset/defects4j-sf.json',
                       help='Path to defects4j-sf.json dataset')
    parser.add_argument('--output', '-o', type=str,
                       default='dataset_test/SRepair/results/sf/defects4j_llm_cfg_patches.json',
                       help='Output path for generated patches')
    parser.add_argument('--validate', '-v', action='store_true',
                       help='Run validation after generating patches')
    parser.add_argument('--validate-only', action='store_true',
                       help='Only run validation on existing patch file (skip patch generation)')
    parser.add_argument('--val-output', type=str, default='dataset_test/SRepair/results/sf/defects4j_llm_cfg_validation',
                       help='Output directory for validation results')
    parser.add_argument('--limit', '-l', type=int, default=None,
                       help='Limit the number of bugs to process (useful for debugging)')
    parser.add_argument('--workers', '-w', type=int, default=None,
                       help='Number of parallel workers (default: CPU count)')
    parser.add_argument('--test-single', '-t', type=str, default=None,
                       help='Test a single bug by name (e.g., Chart-1)')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logger.info("Starting LLM-based Java CFG debug process...")
    logger.info(f"Dataset: {args.dataset}")
    logger.info(f"Output: {args.output}")
    
    # æµ‹è¯•å•ä¸ªbug
    if args.test_single:
        logger.info(f"Testing single bug: {args.test_single}")
        
        # åŠ è½½æ•°æ®é›†
        with open(args.dataset, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        
        if args.test_single not in dataset:
            logger.error(f"Bug {args.test_single} not found in dataset")
            return
        
        # è¿è¡Œå•ä¸ªbugçš„è°ƒè¯•
        corrected_code = llm_java_debug(args.test_single, dataset[args.test_single])
        
        if corrected_code:
            logger.info(f"Generated patch for {args.test_single}")
            
            # ä¿å­˜è¡¥ä¸
            single_output_path = f"{args.test_single}_llm_cfg_patch.json"
            with open(single_output_path, 'w', encoding='utf-8') as f:
                json.dump({
                    args.test_single: {
                        'patches': [corrected_code],
                        'original_buggy': dataset[args.test_single]['buggy'],
                        'bug_info': {
                            'loc': dataset[args.test_single]['loc'],
                            'start': dataset[args.test_single]['start'],
                            'end': dataset[args.test_single]['end']
                        },
                        'patch_generated': True
                    }
                }, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Patch saved to {single_output_path}")
        else:
            logger.warning(f"No patch generated for {args.test_single}")
        
        return
    
    # å¤„ç†å®Œæ•´æ•°æ®é›†
    if args.validate_only:
        logger.info("ğŸ” Validating existing patches only...")
        
        # æ£€æŸ¥è¡¥ä¸æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.output):
            logger.error(f"Patch file not found: {args.output}")
            return
            
        # è¿è¡ŒéªŒè¯
        run_validation(args.output, args.dataset, args.val_output)
        
        # è§£æéªŒè¯ç»“æœ
        statistics = parse_validation_results(args.val_output)
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        stats_file = os.path.join(os.path.dirname(args.output), 'llm_cfg_repair_statistics.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(statistics, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Statistics saved to {stats_file}")
        return
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.dataset):
        logger.error(f"Dataset file not found: {args.dataset}")
        return
    
    # å¤„ç†æ•°æ®é›†
    start_time = time.time()
    results = process_defects4j_dataset_parallel(args.dataset, args.output, args.limit, args.workers)
    processing_time = time.time() - start_time
    
    logger.info(f"Patch generation completed in {processing_time:.2f} seconds")
    
    # è¿è¡ŒéªŒè¯
    if args.validate:
        logger.info("ğŸ” Starting validation with sf_val_d4j...")
        
        # åˆ é™¤ç°æœ‰è¾“å‡ºç›®å½•ä»¥é¿å…å†²çª
        if os.path.exists(args.val_output):
            import shutil
            logger.info(f"Removing existing validation output directory: {args.val_output}")
            shutil.rmtree(args.val_output)
        
        run_validation(args.output, args.dataset, args.val_output)
        
        # è§£æéªŒè¯ç»“æœ
        statistics = parse_validation_results(args.val_output)
        
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        stats_file = os.path.join(os.path.dirname(args.output), 'llm_cfg_repair_statistics.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(statistics, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Statistics saved to {stats_file}")
    
    logger.info("All tasks completed!")

if __name__ == "__main__":
    main() 