#!/usr/bin/env python3
"""
ç›´æ¥è°ƒç”¨å¤§æ¨¡å‹ä¿®å¤ä»£ç çš„å¹¶è¡Œæµ‹è¯•è„šæœ¬
è¿™ä¸ªè„šæœ¬æµ‹è¯•ä»…ä½¿ç”¨å¤§æ¨¡å‹ç›´æ¥ä¿®å¤ä»£ç èƒ½è¾¾åˆ°çš„å‡†ç¡®åº¦ï¼Œæ— éœ€å¤æ‚çš„CFGåˆ†æ
"""

import json
import os
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
from loguru import logger
from pathlib import Path
from typing import Dict, Any, Tuple
from tqdm import tqdm
from chat import direct_fix_code,ai_critic_word
from utils import extract_buggy_code, run_check_function

def setup_logger_for_process(process_id: int):
    """ä¸ºæ¯ä¸ªè¿›ç¨‹è®¾ç½®ç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶"""
    log_file = f"direct_fix_log_process_{process_id}.txt"
    logger.add(log_file, encoding="utf-8", rotation="10 MB", retention="10 days")

def direct_fix_single_task(task_data: Tuple[int, str]) -> Tuple[int, bool, str]:
    """
    å¤„ç†å•ä¸ªä»£ç ä¿®å¤ä»»åŠ¡
    Args:
        task_data: (task_index, task_json_line)
    Returns:
        (task_index, is_success, error_message)
    """
    task_idx, line = task_data
    # if task_idx != 54 and task_idx != 163:
    #     return task_idx, False, "è·³è¿‡é54å’Œ163ä»»åŠ¡"
    process_id = os.getpid()
    
    # ä¸ºå½“å‰è¿›ç¨‹è®¾ç½®æ—¥å¿—
    setup_logger_for_process(process_id)
    
    try:
        logger.info(f"=================ç›´æ¥ä¿®å¤ä»»åŠ¡ {task_idx}=================")
        task = json.loads(line)
        function_name = task['entry_point']
        
        # åˆ›å»ºå”¯ä¸€çš„ä¸´æ—¶ä»£ç æ–‡ä»¶
        buggy_code_file = f"direct_buggy_proc_{process_id}_{task_idx}.py"
        
        # æå–é”™è¯¯ä»£ç 
        buggy_code = extract_buggy_code(task, buggy_code_file)
        task_description = task['docstring']
        
        # è·å–ç¤ºä¾‹æµ‹è¯•ç”¨ä¾‹ - ä½¿ç”¨ç¬¬ä¸€ä¸ªexample testä½œä¸ºå‚è€ƒ
        example_test = task['example_test']
        
        logger.info(f"å‡½æ•°å: {function_name}")
        logger.info(f"ä»»åŠ¡æè¿°: {task_description}")
        logger.info(f"ç¤ºä¾‹æµ‹è¯•: {example_test}")
        
        # ç›´æ¥è°ƒç”¨å¤§æ¨¡å‹ä¿®å¤ä»£ç 
        logger.info("å¼€å§‹è°ƒç”¨å¤§æ¨¡å‹ä¿®å¤ä»£ç ...")
        recheckDetails = ""
        for i in range(3):
            buggy_code = direct_fix_code(task_description, example_test, buggy_code,recheckDetails)
            is_success = run_check_function(function_name, example_test, buggy_code)
            # if is_success:
            #     break
            ispassed,recheckDetails= ai_critic_word(task_description, example_test, buggy_code)
            if ispassed:
                break

        
        logger.info(f"ä¿®å¤åçš„ä»£ç :\n{buggy_code}")
        
        # ä½¿ç”¨éšè—æµ‹è¯•ç”¨ä¾‹éªŒè¯ä¿®å¤ç»“æœ
        hidden_check_test = task['test']
        logger.info("å¼€å§‹éªŒè¯ä¿®å¤ç»“æœ...")
        is_success = run_check_function(function_name, hidden_check_test, buggy_code)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if os.path.exists(buggy_code_file):
                os.remove(buggy_code_file)
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {buggy_code_file}: {e}")
        
        result_msg = "âœ… ä¿®å¤æˆåŠŸ" if is_success else "âŒ ä¿®å¤å¤±è´¥"
        logger.info(f"ä»»åŠ¡ {task_idx} {result_msg}")
        
        return task_idx, is_success, ""
        
    except Exception as e:
        error_msg = f"ä»»åŠ¡ {task_idx} å¤„ç†å¼‚å¸¸: {str(e)}"
        logger.error(error_msg)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            buggy_code_file = f"direct_buggy_proc_{process_id}_{task_idx}.py"
            if os.path.exists(buggy_code_file):
                os.remove(buggy_code_file)
        except:
            pass
            
        return task_idx, False, error_msg

def direct_fix_all_tasks_parallel(dataset_path: str, max_workers: int = None):
    """
    å¹¶è¡Œå¤„ç†æ‰€æœ‰ä»£ç ä¿®å¤ä»»åŠ¡
    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
    """
    # è¯»å–æ‰€æœ‰ä»»åŠ¡
    with open(dataset_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    total_tasks = len(lines)
    logger.info(f"æ€»ä»»åŠ¡æ•°: {total_tasks}")
    
    # å‡†å¤‡ä»»åŠ¡æ•°æ®
    task_data_list = [(idx, line) for idx, line in enumerate(lines)]
    
    # è®¾ç½®å¹¶è¡Œåº¦
    if max_workers is None:
        max_workers = mp.cpu_count()
    
    logger.info(f"ä½¿ç”¨ {max_workers} ä¸ªå¹¶è¡Œè¿›ç¨‹")
    
    success_count = 0
    completed_count = 0
    results = {}
    error_details = {}
    
    # ä½¿ç”¨è¿›ç¨‹æ± æ‰§è¡Œå¹¶è¡Œå¤„ç†
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {executor.submit(direct_fix_single_task, task_data): task_data[0] 
                         for task_data in task_data_list}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        progress_bar = tqdm(as_completed(future_to_task), total=total_tasks, desc="ç›´æ¥ä¿®å¤ä»»åŠ¡")
        
        for future in progress_bar:
            task_idx = future_to_task[future]
            try:
                task_idx_result, is_success, error_msg = future.result()
                results[task_idx_result] = is_success
                if error_msg:
                    error_details[task_idx_result] = error_msg
                    
                if is_success:
                    success_count += 1
                completed_count += 1
                
                # æ›´æ–°è¿›åº¦æ¡æè¿°
                current_rate = success_count / completed_count
                progress_bar.set_description(f"ç›´æ¥ä¿®å¤ä»»åŠ¡ (æˆåŠŸç‡: {current_rate:.1%})")
                
                # å®æ—¶æ˜¾ç¤ºå½“å‰æˆåŠŸç‡
                if completed_count % 10 == 0 or completed_count == total_tasks:
                    print(f"\nå½“å‰è¿›åº¦: {success_count} / {completed_count} ({current_rate:.2%})")
                
            except Exception as e:
                logger.error(f"ä»»åŠ¡ {task_idx} æ‰§è¡Œå¼‚å¸¸: {e}")
                results[task_idx] = False
                error_details[task_idx] = f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"
                completed_count += 1

    # æœ€ç»ˆç»Ÿè®¡
    final_success_rate = success_count / total_tasks
    print(f"\n{'='*50}")
    print(f"ğŸ¯ ç›´æ¥å¤§æ¨¡å‹ä¿®å¤ - æœ€ç»ˆç»“æœ")
    print(f"{'='*50}")
    print(f"ğŸ“Š æ€»ä»»åŠ¡æ•°: {total_tasks}")
    print(f"âœ… æˆåŠŸä»»åŠ¡: {success_count}")
    print(f"âŒ å¤±è´¥ä»»åŠ¡: {total_tasks - success_count}")
    print(f"ğŸ¯ æœ€ç»ˆæˆåŠŸç‡: {final_success_rate:.2%}")
    print(f"{'='*50}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_summary = {
        "experiment_type": "direct_llm_fix",
        "total_tasks": total_tasks,
        "success_count": success_count,
        "failure_count": total_tasks - success_count,
        "success_rate": final_success_rate,
        "detailed_results": results,
        "error_details": error_details,
        "max_workers": max_workers
    }
    
    results_file = "direct_fix_results.json"
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(results_summary, f, indent=2, ensure_ascii=False)
    
    logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° {results_file}")
    
    # åˆ†æå¤±è´¥æ¡ˆä¾‹
    failed_tasks = [idx for idx, success in results.items() if not success]
    if failed_tasks:
        print(f"\nâŒ å¤±è´¥ä»»åŠ¡ç´¢å¼•: {failed_tasks[:10]}{'...' if len(failed_tasks) > 10 else ''}")
        if error_details:
            print(f"ğŸ“ éƒ¨åˆ†é”™è¯¯è¯¦æƒ…:")
            for idx, error in list(error_details.items())[:5]:
                print(f"  ä»»åŠ¡{idx}: {error}")
    
    return final_success_rate

if __name__ == "__main__":
    # è®¾ç½®ä¸»è¿›ç¨‹æ—¥å¿—
    logger.add("direct_fix_main.txt", encoding="utf-8", rotation="10 MB", retention="10 days")
    
    dataset_path = "./dataset_test/humanevalfix/humanevalpack.jsonl"
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    if not os.path.exists(dataset_path):
        print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
        exit(1)
    
    print("ğŸš€ ç›´æ¥å¤§æ¨¡å‹ä»£ç ä¿®å¤å¹¶è¡Œæµ‹è¯•")
    print("=" * 50)
    max_workers = int(os.environ.get("MAX_WORKERS", mp.cpu_count()))
    logger.info(f"å¼€å§‹å®Œæ•´å¹¶è¡Œæµ‹è¯•ï¼Œä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹")
    final_rate = direct_fix_all_tasks_parallel(dataset_path, max_workers)
    print(f"\nğŸ å®éªŒå®Œæˆï¼ç›´æ¥å¤§æ¨¡å‹ä¿®å¤æˆåŠŸç‡: {final_rate:.2%}") 